{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is the application of machine learning to text and language problems. It is amongst the fastest growing areas of the data science domain and has seen a meteroic rise in activity since the invention of word2vec in 2013. While many NLP analysis generally faces the same challenges of more straightforward machine learning models, they also have a set of totally unique problems.\n",
    "\n",
    "**What are some potential challenges with working with text data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "Often we're required to extract the data ourselves from a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'It is never this easy.'\n"
     ]
    }
   ],
   "source": [
    "txt_path = '../data/demo_text.txt'\n",
    "\n",
    "file1 = open(txt_path,\"rb\") \n",
    "txt_text = file1.read()\n",
    "file1.close() \n",
    "\n",
    "print(txt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_text(path):\n",
    "    #import io\n",
    "    from xml.etree.cElementTree import XML\n",
    "    import zipfile\n",
    "    \n",
    "    WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "    PARA = WORD_NAMESPACE + 'p'\n",
    "    TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "    ## Control for Path VS Bytes object from zip    \n",
    "    #if isinstance(path, io.BytesIO):\n",
    "        #path.seek(0)\n",
    "\n",
    "    document = zipfile.ZipFile(path)        \n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)\n",
    "\n",
    "docx_path = '../../pre_work/git_github_ve_instructions/2_Virtual_Environment_Setup/Virtual_Environments_Setup.docx'\n",
    "\n",
    "docx_to_text(docx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "\n",
    "pdf_path = '../data/_sreo1015pdf.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read binary pdf object into memory and parse with PyPDF2\n",
    "\n",
    "pdfFileObj = open(pdf_path, 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pages\n",
    "pdfReader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "for page in range(pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(page)\n",
    "    text = pageObj.extractText().strip().replace('\\n',' ')\n",
    "    full_text.append(text+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pdfminer = convert_pdf_to_txt(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pdfminer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometimes the situation is more dire, what about scanned documents?\n",
    "\n",
    "Optical character recognition can process images and uses computer vision models to find and transcribe the typed print and put letters together to words and words together into \"blobs\" of text. The utility tesseract and its python wrapper pyTesseract is the primary OCR tool available in Python.\n",
    "\n",
    "\n",
    "What are some pitfalls of OCR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "There's several steps for preparing our data for use in any NLP activities.\n",
    "\n",
    "0) Pre-cleaning / data specific noise removal.\n",
    "\n",
    "1) Tokenization\n",
    "\n",
    "2) Cleaning\n",
    "\n",
    "3) Lemmatization **never stemming**\n",
    "\n",
    "4) Stop word removal (optional)\n",
    "\n",
    "5) Phrase parsing (optional)\n",
    "\n",
    "6) Part-of-speech tagging (optional ... kind of)\n",
    "\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Tokenization is the segmententation of text it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off â€“ whereas \"U.K.\" should remain one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/hp_data.csv', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['text'].apply(lambda x: nlp(x, disable = ['ner']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words like \"the\" or \"or\" or pronouns that add little substantive value to a document. While they can provide clues about relationships of words, for many activities like topic classification or phrase comparison they largely just serve as noise. Removal of stopwords is very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print('Example stop words: {}'.format(list(STOP_WORDS)[0:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "From Stanford NLTK group \n",
    "\n",
    "\"Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.tokenized[50]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
    "    'TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "# print various SpaCy POS attributes\n",
    "for token in example:\n",
    "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
    "          token.text, token.lemma_, token.pos_, token.tag_, token.dep_\n",
    "        , token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams and Phrase Parsing\n",
    "\n",
    "The tokenization of single words is a unigram. N-grams is the general class of joined text. 2 and 3 word phrases can offer different meaning than the words individually. For example the bigrams \"Union Station\" or \"Chicago Bulls\" have very different meanings than just the two individual unigrams. Finding these in your document and converting unigrams to n-grams can be done by what is called phrase parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[2369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_preprocess(df.text[2369]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_text = [simple_preprocess(text) for text in df.text]\n",
    "\n",
    "common_terms = list(STOP_WORDS)\n",
    "\n",
    "phrases = Phrases(\n",
    "      gensim_text\n",
    "    , common_terms=common_terms\n",
    "    , min_count=10\n",
    "    , threshold=5\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "        are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))\n",
    "    \n",
    "print_phrases(bigram, gensim_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "      bigram[gensim_text]\n",
    "    , common_terms=common_terms\n",
    "    , min_count=3\n",
    "    , threshold=1\n",
    ")\n",
    "\n",
    "trigram = Phraser(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_num in [2369]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(gensim_text[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[gensim_text[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[gensim_text[doc_num]]])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['out_text'] = out_text\n",
    "df[['line', 'text', 'book', 'text2', 'chapter','out_text']].to_pickle('../data/hp_processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
