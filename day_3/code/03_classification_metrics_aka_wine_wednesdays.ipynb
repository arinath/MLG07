{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = pd.read_csv('../data/winequality-red.csv', delimiter=';')\n",
    "df_white = pd.read_csv('../data/winequality-white.csv', delimiter=';')\n",
    "\n",
    "df_red['type'] = 'red'\n",
    "print(str(df_red.shape[0]) + ' red wines.')\n",
    "\n",
    "df_white['type'] = 'white'\n",
    "print(str(df_white.shape[0]) + ' white wines.')\n",
    "\n",
    "df = pd.concat([df_red,df_white])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_model_data = df.drop('quality', axis = 1)\n",
    "quality_model_data = df\n",
    "\n",
    "y_type = (type_model_data.pop('type') == 'red')*1\n",
    "y_quality = quality_model_data.pop('quality')\n",
    "\n",
    "y_quality_7 = (y_quality>= 7)*1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(type_model_data, y_type,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'none', max_iter = 5000)\n",
    "lr.fit(X_train,\n",
    "        y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "print('--Train--')\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('--Test--')\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_model_data['type'] = (quality_model_data['type']=='red')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(quality_model_data, y_quality_7,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'none', max_iter = 5000)\n",
    "lr.fit(X_train,\n",
    "        y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "print(' --Train--\\n')\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(' --Test-- \\n')\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is the sum of the correctly predicted classes divided by the total number of observations it made a prediction for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a useful measure here? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, np.repeat(0,len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "With class imbalance, accuracy is note always a useful metric to evaluate the quality of a model. Consider the extreme case of credit card fraud. The most accurate model may be just to say nothing is fraud, but\n",
    "\n",
    "**Recall** refers to the percentage of a class that was correctly classified as that class. $\\frac{True  Positives}{True  Positive  + False  Negatives}$\n",
    "\n",
    "**Precision** refers to the percentage of observations classified as a class, were actually that call.$\\frac{True Positives}{True  Positive  + False Positives}$\n",
    "\n",
    "**F1-Score** F1 is the harmonic mean of precision and recall. A harmonic mean is a more meaningful average for rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding\n",
    "\n",
    "In binary classification we do have the flexibility to say that we'll change classification thresholds. The default says if p >= .5, then call it a one.  But we can experiment with that to try and get better performance from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = lr.predict_proba(X_train)\n",
    "y_prob_test = lr.predict_proba(X_test)\n",
    "\n",
    "y_pred_thresh = (y_prob[:,1] > .85)*1\n",
    "y_pred_test_thresh = (y_prob_test[:,1] > .)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "plot_precision_recall_curve(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC Curve\n",
    "\n",
    "Area under the curve, receiver operating characteristics is another highly important metrics to evaluate your model. This value provides information about how well the model can distinguish between classes, so a high value denotes a good model.  There is 1 ROC curve per class, but averages of them can be taken to consolidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/ROCAUC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_test[:,1])\n",
    "fpr0, tpr0, _ = roc_curve(y_test, y_prob_test[:,0])\n",
    "\n",
    "\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for Working with Unbalanced Data\n",
    "\n",
    "There are many different techniques for handling an imbalance in the data. Three common techniques are class weighting, oversampling, and undersampling.\n",
    "\n",
    "**Class weighting** lets you change how much misclassification of that class impacts the loss function. In binary classification, you it's common practice to make the majority class weight 1 and the minority class weight = 1/(% in class).\n",
    "\n",
    "**Oversampling** involves increasing the prevalance of minority class(es) so that the dataset has roughly balanced classes. While this can be effective, it does reduce the variability of the minority class(es) and can lead to overfitting if not done carefully. SMOTE is a common approach to this.\n",
    "\n",
    "**Undersampling** is where a random selection of the majority class(es) is taken with size roughly equal to the minority class.  If there isn't enough data available this can prove difficult.\n",
    "\n",
    "\n",
    "### Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(penalty = 'none', class_weight='balanced',max_iter = 5000)\n",
    "\n",
    "lr_balanced.fit(X_train,\n",
    "        y_train)\n",
    "\n",
    "y_pred = lr_balanced.predict(X_train)\n",
    "y_pred_test = lr_balanced.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = len(y_train)-sum(y_train)\n",
    "print(delta)\n",
    "df_over_sampled = X_train[y_train==1].sample(delta, random_state=42, replace = True)\n",
    "\n",
    "X_train_over = pd.concat([X_train, df_over_sampled])\n",
    "y_train_over = pd.concat([y_train, pd.Series(np.repeat(1,delta))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_over = LogisticRegression(penalty = 'none', class_weight='balanced',max_iter = 5000)\n",
    "\n",
    "lr_over.fit(X_train_over,\n",
    "        y_train_over)\n",
    "\n",
    "y_pred = lr_over.predict(X_train)\n",
    "y_pred_test = lr_over.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_minority = sum(y_train)\n",
    "df_under_sampled = X_train[y_train==0].sample(num_minority, random_state=42, replace = True)\n",
    "df_under_sampled_min = X_train[y_train==1]\n",
    "\n",
    "\n",
    "X_train_under = pd.concat([df_under_sampled, df_under_sampled_min])\n",
    "y_train_under = pd.concat([pd.Series(np.repeat(0,num_minority)), pd.Series(np.repeat(1,num_minority))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_over = LogisticRegression(penalty = 'none', class_weight='balanced',max_iter = 5000)\n",
    "\n",
    "lr_over.fit(X_train_under,\n",
    "        y_train_under)\n",
    "\n",
    "y_pred = lr_over.predict(X_train)\n",
    "y_pred_test = lr_over.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
